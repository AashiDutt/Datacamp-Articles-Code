{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcKjrJk15fIubosT78hRSa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AashiDutt/Datacamp-Articles-Code/blob/main/Speculative_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch datasets"
      ],
      "metadata": {
        "id": "_q3zPLTasCla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "dataset = load_dataset(\"squad\", split=\"train[:100]\")  # Using a small subset for demonstration\n"
      ],
      "metadata": {
        "id": "3HrrEdSAsD0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "# Initialize the smaller model (RAG Drafter)\n",
        "drafter_model_name = \"distilbert-base-uncased-distilled-squad\"\n",
        "drafter_model = AutoModelForQuestionAnswering.from_pretrained(drafter_model_name)\n",
        "drafter_tokenizer = AutoTokenizer.from_pretrained(drafter_model_name)\n",
        "\n",
        "# Initialize the larger model (RAG Verifier)\n",
        "verifier_model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "verifier_model = AutoModelForQuestionAnswering.from_pretrained(verifier_model_name)\n",
        "verifier_tokenizer = AutoTokenizer.from_pretrained(verifier_model_name)\n",
        "\n",
        "# Set up pipelines\n",
        "drafter_pipeline = pipeline(\"question-answering\", model=drafter_model, tokenizer=drafter_tokenizer)\n",
        "verifier_pipeline = pipeline(\"question-answering\", model=verifier_model, tokenizer=verifier_tokenizer)\n"
      ],
      "metadata": {
        "id": "ypREVvPPsJ_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_drafts(question, context, num_drafts=3):\n",
        "    drafts = []\n",
        "    for _ in range(num_drafts):\n",
        "        draft = drafter_pipeline(question=question, context=context)\n",
        "        drafts.append(draft)\n",
        "    return drafts\n"
      ],
      "metadata": {
        "id": "Yka1swCdsNSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_drafts(question, context, drafts):\n",
        "    best_draft = None\n",
        "    highest_score = 0\n",
        "\n",
        "    # Tokenize the context using the verifier's tokenizer, keeping track of offsets\n",
        "    inputs = verifier_tokenizer(question, context, return_tensors=\"pt\", return_offsets_mapping=True)\n",
        "    offset_mapping = inputs['offset_mapping'][0]  # This will give us the character-to-token mapping\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "\n",
        "    for draft in drafts:\n",
        "        start_char = draft['start']\n",
        "        end_char = draft['end']\n",
        "\n",
        "        # Find the corresponding token positions using offset mapping\n",
        "        start_index = None\n",
        "        end_index = None\n",
        "\n",
        "        for idx, (start, end) in enumerate(offset_mapping):\n",
        "            if start_index is None and start_char >= start and start_char < end:\n",
        "                start_index = idx\n",
        "            if end_index is None and end_char > start and end_char <= end:\n",
        "                end_index = idx\n",
        "            if start_index is not None and end_index is not None:\n",
        "                break\n",
        "\n",
        "        # Ensure indices were found and are within bounds\n",
        "        if start_index is None or end_index is None or start_index >= len(input_ids) or end_index >= len(input_ids):\n",
        "            print(f\"Draft skipped: Out of bounds or no matching tokens. Start Index: {start_index}, End Index: {end_index}\")\n",
        "            continue\n",
        "\n",
        "        # Get the confidence score using the larger model\n",
        "        outputs = verifier_model(input_ids=input_ids.unsqueeze(0))\n",
        "        score = outputs.start_logits[0, start_index].item() + outputs.end_logits[0, end_index].item()\n",
        "\n",
        "        if score > highest_score:\n",
        "            highest_score = score\n",
        "            best_draft = draft\n",
        "\n",
        "    if best_draft is None:\n",
        "        print(\"No valid draft found after verification.\")\n",
        "    return best_draft\n"
      ],
      "metadata": {
        "id": "DkOmYnkTsOyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 10  # Evaluate on 10 samples for simplicity\n",
        "\n",
        "for i in range(total):\n",
        "    sample = dataset[i]\n",
        "    question = sample['question']\n",
        "    context = sample['context']\n",
        "\n",
        "    drafts = generate_drafts(question, context)\n",
        "    best_answer = verify_drafts(question, context, drafts)\n",
        "\n",
        "    print(f\"Q: {question}\")\n",
        "\n",
        "    if best_answer is not None:\n",
        "        print(f\"A: {best_answer['answer']}\\n\")\n",
        "\n",
        "        # For simplicity, compare with the first answer (gold) provided in the dataset\n",
        "        if best_answer['answer'].lower() in sample['answers']['text'][0].lower():\n",
        "            correct += 1\n",
        "    else:\n",
        "        print(\"No valid draft found.\\n\")\n",
        "\n",
        "accuracy = correct / total * 100\n",
        "print(f\"Accuracy: {accuracy}%\")\n"
      ],
      "metadata": {
        "id": "E6nWqwM6uBWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}